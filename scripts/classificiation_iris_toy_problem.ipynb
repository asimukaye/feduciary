{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Toy problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_classification, make_moons, make_circles\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor, Tensor\n",
    "from torch.nn import Sequential, Linear, BCELoss, Sigmoid, Tanh, Module, CrossEntropyLoss, ReLU, Softmax, NLLLoss, KLDivLoss, L1Loss, MSELoss\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, Adagrad, Adadelta, AdamW, RMSprop, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self) -> None:\n",
    "        self.history = {}\n",
    "        self.epoch = []\n",
    "\n",
    "def model_forward_func(model: Module):\n",
    "    return lambda x: model(torch.tensor(x, dtype=torch.float32)).detach().squeeze().numpy()\n",
    "        \n",
    "def to_categorical(x, num_classes=None):\n",
    "    \"\"\"Converts a class vector (integers) to binary class matrix.\n",
    "\n",
    "    E.g. for use with `categorical_crossentropy`.\n",
    "\n",
    "    Args:\n",
    "        x: Array-like with class values to be converted into a matrix\n",
    "            (integers from 0 to `num_classes - 1`).\n",
    "        num_classes: Total number of classes. If `None`, this would be inferred\n",
    "            as `max(x) + 1`. Defaults to `None`.\n",
    "\n",
    "    Returns:\n",
    "        A binary matrix representation of the input as a NumPy array. The class\n",
    "        axis is placed last.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> a = keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n",
    "    >>> print(a)\n",
    "    [[1. 0. 0. 0.]\n",
    "     [0. 1. 0. 0.]\n",
    "     [0. 0. 1. 0.]\n",
    "     [0. 0. 0. 1.]]\n",
    "\n",
    "    >>> b = np.array([.9, .04, .03, .03,\n",
    "    ...               .3, .45, .15, .13,\n",
    "    ...               .04, .01, .94, .05,\n",
    "    ...               .12, .21, .5, .17],\n",
    "    ...               shape=[4, 4])\n",
    "    >>> loss = keras.ops.categorical_crossentropy(a, b)\n",
    "    >>> print(np.around(loss, 5))\n",
    "    [0.10536 0.82807 0.1011  1.77196]\n",
    "\n",
    "    >>> loss = keras.ops.categorical_crossentropy(a, a)\n",
    "    >>> print(np.around(loss, 5))\n",
    "    [0. 0. 0. 0.]\n",
    "    \"\"\"\n",
    "    x = np.array(x, dtype=\"int64\")\n",
    "    input_shape = x.shape\n",
    "\n",
    "    # Shrink the last dimension if the shape is (..., 1).\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "\n",
    "    x = x.reshape(-1)\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(x) + 1\n",
    "    batch_size = x.shape[0]\n",
    "    categorical = np.zeros((batch_size, num_classes))\n",
    "    categorical[np.arange(batch_size), x] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.axes import Axes\n",
    "\n",
    "def plot_decision_boundary(func, X, y, figsize=(7, 5), ax: Axes = None):\n",
    "    amin, bmin = X.min(axis=0) - 0.1\n",
    "    amax, bmax = X.max(axis=0) + 0.1\n",
    "    hticks = np.linspace(amin, amax, 101)\n",
    "    vticks = np.linspace(bmin, bmax, 101)\n",
    "    \n",
    "    aa, bb = np.meshgrid(hticks, vticks)\n",
    "    ab = np.c_[aa.ravel(), bb.ravel()]\n",
    "    c = func(ab)\n",
    "    cc = c.reshape(aa.shape)\n",
    "\n",
    "    cm = plt.colormaps['RdBu']\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "    contour = ax.contourf(aa, bb, cc, cmap=cm, alpha=0.8)\n",
    "    \n",
    "    ax_c = plt.colorbar(contour, ax=ax)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "    \n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    ax.set_xlim(amin, amax)\n",
    "    ax.set_ylim(bmin, bmax)\n",
    "    ax.set_title(\"Decision Boundary\")\n",
    "    return ax\n",
    "\n",
    "def plot_multiclass_decision_boundary(model: Module, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n",
    "    cmap = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "    Z = model.forward(tensor(np.c_[xx.ravel(), yy.ravel()])).detach().numpy()\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.colormaps['Spectral'], alpha=0.8)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.colormaps['RdYlBu'])\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    return ax\n",
    "    \n",
    "def plot_data(X, y, figsize=(6, 4)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(X[y==0, 0], X[y==0, 1], 'or', alpha=0.5, label=0)\n",
    "    ax.plot(X[y==1, 0], X[y==1, 1], 'ob', alpha=0.5, label=1)\n",
    "    ax.set_xlim((min(X[:, 0])-0.1, max(X[:, 0])+0.1))\n",
    "    ax.set_ylim((min(X[:, 1])-0.1, max(X[:, 1])+0.1))\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "def plot_loss_accuracy(history: History, ax: Axes = None):\n",
    "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(6, 4))\n",
    "        ax = fig.add_subplot(111)\n",
    "    historydf.plot(ylim=(0, max(1, historydf.values.max())), ax=ax)\n",
    "    loss = history.history['loss'][-1]\n",
    "    acc = history.history['acc'][-1]\n",
    "    ax.set_title('Loss: %.3f, Accuracy: %.3f' % (loss, acc))\n",
    "    return  ax\n",
    "\n",
    "def plot_loss(history: History):\n",
    "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    historydf.plot(ylim=(0, historydf.values.max()), ax=ax)\n",
    "    fig.suptitle('Loss: %.3f' % history.history['loss'][-1])\n",
    "    return fig, ax\n",
    "\n",
    "def plot_confusion_matrix(y_pred, y, ax: Axes = None):\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(6, 4))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "    sns.heatmap(pd.DataFrame(confusion_matrix(y, y_pred)), annot=True, fmt='d', cmap='YlGnBu', alpha=0.8, vmin=0, ax=ax)\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    return ax\n",
    "\n",
    "def plot_compare_histories(history_list: list[History], name_list, plot_accuracy=True):\n",
    "    dflist = []\n",
    "    for history in history_list:\n",
    "        h = {key: val for key, val in history.history.items() if not key.startswith('val_')}\n",
    "        dflist.append(pd.DataFrame(h, index=history.epoch))\n",
    "\n",
    "    historydf = pd.concat(dflist, axis=1)\n",
    "\n",
    "    metrics = dflist[0].columns\n",
    "    idx = pd.MultiIndex.from_product([name_list, metrics], names=['model', 'metric'])\n",
    "    historydf.columns = idx\n",
    "    \n",
    "    fig = plt.figure(figsize=(6, 8))\n",
    "\n",
    "    ax = fig.add_subplot(211)\n",
    "    historydf.xs('loss', axis=1, level='metric').plot(ylim=(0,1), ax=ax)\n",
    "    ax.set_title(\"Loss\")\n",
    "    \n",
    "    if plot_accuracy:\n",
    "        ax = fig.add_subplot(212)\n",
    "        historydf.xs('acc', axis=1, level='metric').plot(ylim=(0,1), ax=ax)\n",
    "        ax.set_title(\"Accuracy\")\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def make_sine_wave():\n",
    "    c = 3\n",
    "    num = 2400\n",
    "    step = num/(c*4)\n",
    "    np.random.seed(0)\n",
    "    x0 = np.linspace(-c*np.pi, c*np.pi, num)\n",
    "    x1 = np.sin(x0)\n",
    "    noise = np.random.normal(0, 0.1, num) + 0.1\n",
    "    noise = np.sign(x1) * np.abs(noise)\n",
    "    x1  = x1 + noise\n",
    "    x0 = x0 + (np.asarray(range(num)) / step) * 0.3\n",
    "    X = np.column_stack((x0, x1))\n",
    "    y = np.asarray([int((i/step)%2==1) for i in range(len(x0))])\n",
    "    return X, y\n",
    "\n",
    "def make_multiclass(N=500, D=2, K=3):\n",
    "    \"\"\"\n",
    "    N: number of points per class\n",
    "    D: dimensionality\n",
    "    K: number of classes\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    X = np.zeros((N*K, D))\n",
    "    y = np.zeros(N*K)\n",
    "    for j in range(K):\n",
    "        ix = range(N*j, N*(j+1))\n",
    "        # radius\n",
    "        r = np.linspace(0.0,1,N)\n",
    "        # theta\n",
    "        t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.colormaps['RdYlBu'], alpha=0.8)\n",
    "    ax.set_xlim(-1,1)\n",
    "    ax.set_ylim(-1,1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_data(plot=False):\n",
    "    X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
    "                           n_informative=2, random_state=7, n_clusters_per_class=1)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    if plot:\n",
    "        plot_data(X, y)\n",
    "    return X, y, X_tensor, y_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y, X_ten, y_ten = get_classification_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Sklearn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sklearn_logistic_regression(X, y):\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, y)\n",
    "    print('LR coefficients:', lr.coef_)\n",
    "    print('LR intercept:', lr.intercept_)\n",
    "\n",
    "    plot_data(X, y)\n",
    "\n",
    "    limits = np.array([-2, 2])\n",
    "    boundary = -(lr.coef_[0][0] * limits + lr.intercept_[0]) / lr.coef_[0][1]\n",
    "    plt.plot(limits, boundary, \"g-\", linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logistic_regression_model():\n",
    "    model = Sequential(Linear(2, 1), Sigmoid())\n",
    "    criterion = BCELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.1)\n",
    "    return model, criterion, optimizer\n",
    "# model = Sequential(Linear(2, 1), Sigmoid())\n",
    "# criterion = BCELoss()\n",
    "# optimizer = SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model: Module, X_tensor: Tensor, y_tensor: Tensor, optimizer, criterion, epochs: int):\n",
    "    history = History()\n",
    "    history.history['loss'] = []\n",
    "    history.history['acc'] = []\n",
    "    # history.history['acc2'] = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_tensor)\n",
    "        loss = criterion(y_pred, y_tensor.view(-1, 1))\n",
    "        # print(y_tensor.shape)\n",
    "        # print(y_pred.shape)\n",
    "\n",
    "        # print(y_tensor.view(-1,1).shape)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_pred_d = y_pred > 0.5\n",
    "        acc = (y_pred_d == y_tensor.view(-1, 1)).sum().item() / len(y_tensor)\n",
    "\n",
    "        # total = 0\n",
    "        # correct = 0\n",
    "        # total += y_tensor.size(0)\n",
    "        # correct += torch.sum(y_pred.round().detach() == y_tensor.view(-1,1)).item()\n",
    "        # accuracy = correct/total\n",
    "        \n",
    "        # history.history['acc2'].append(accuracy)\n",
    "        history.history['loss'].append(loss.item())\n",
    "        history.history['acc'].append(acc)\n",
    "        history.epoch.append(epoch)\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch:', epoch, 'Loss:', loss.item())\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log regr for simple classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_tensor, y_tensor = get_classification_data()\n",
    "model, criterion, optimizer = get_logistic_regression_model()\n",
    "model, history = trainer(model, X_tensor, y_tensor, optimizer, criterion, 50)\n",
    "y_pred = (model(X_tensor) > 0.5).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_fig, axs = plt.subplots(1,3 ,figsize=(16, 4), width_ratios=[6,8,6])\n",
    "ax1 = plot_loss_accuracy(history, ax=axs[0])\n",
    "ax2 = plot_decision_boundary(model_forward_func(model), X, y, ax=axs[1])\n",
    "ax3 = plot_confusion_matrix(y_pred, y, ax=axs[2])\n",
    "# print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moons_data(plot=False):\n",
    "    X, y = make_moons(n_samples=1000, noise=0.05, random_state=0)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    if plot:\n",
    "        plot_data(X, y)\n",
    "    return X, y, X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential(Linear(2, 1), Sigmoid())\n",
    "criterion = BCELoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "model, history = trainer(model, X_tensor, y_tensor, optimizer, criterion, 100)\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(model_forward_func(model), X, y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(X_tensor).squeeze().round().detach().numpy()\n",
    "print(classification_report(y, y_pred))\n",
    "plot_confusion_matrix(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    Linear(2,4),\n",
    "    Tanh(),\n",
    "    Linear(4,2),\n",
    "    Tanh(),\n",
    "    Linear(2,1),\n",
    "    Sigmoid()\n",
    ")\n",
    "criterion = BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.1)\n",
    "model, history = trainer(model, X_tensor, y_tensor, optimizer, criterion, 100)\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(model_forward_func(model), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=1000, noise=0.05, factor=0.3, random_state=0)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    Linear(2,4),\n",
    "    Tanh(),\n",
    "    Linear(4,2),\n",
    "    Tanh(),\n",
    "    Linear(2,1),\n",
    "    Sigmoid()\n",
    ")\n",
    "criterion = BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.1)\n",
    "model, history = trainer(model, X_tensor, y_tensor, optimizer, criterion, 100)\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(model_forward_func(model), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_sine_wave()\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_multiclass(K=3)\n",
    "y_cat = to_categorical(y)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_cat, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch\n",
    "model = Sequential(Linear(2, 3))\n",
    "criterion = BCELoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "model, history = trainer(model, X_tensor, y_tensor, optimizer, criterion, 100)\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\n",
    "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n",
    "_ = ax.legend(\n",
    "    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused but required import for doing 3d projections with matplotlib < 3.2\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\", elev=-150, azim=110)\n",
    "\n",
    "X_reduced = PCA(n_components=3).fit_transform(iris.data)\n",
    "ax.scatter(\n",
    "    X_reduced[:, 0],\n",
    "    X_reduced[:, 1],\n",
    "    X_reduced[:, 2],\n",
    "    c=iris.target,\n",
    "    s=40,\n",
    ")\n",
    "\n",
    "ax.set_title(\"First three PCA dimensions\")\n",
    "ax.set_xlabel(\"1st Eigenvector\")\n",
    "ax.xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"2nd Eigenvector\")\n",
    "ax.yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"3rd Eigenvector\")\n",
    "ax.zaxis.set_ticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()\n",
    "history.history['loss'] = []\n",
    "history.history['acc'] = []\n",
    "for e in range(50):\n",
    "    \n",
    "    y_pred_tensor = torch.squeeze(model(X_tensor))\n",
    "    loss = criterion(y_pred_tensor, y_tensor)\n",
    "    y_pred = y_pred_tensor.round().detach().numpy()\n",
    "    \n",
    "    if e % 10 == 0:\n",
    "        print('Epoch:', e, 'Loss:', loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total += y_tensor.size(0)\n",
    "    correct += np.sum(y_pred == y)\n",
    "    accuracy = correct/total\n",
    "\n",
    "    history.history['loss'].append(loss.item())\n",
    "    history.history['acc'].append(accuracy)\n",
    "\n",
    "    history.epoch.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
