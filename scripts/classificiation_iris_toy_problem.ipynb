{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Toy problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from icecream import ic\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_classification, make_moons, make_circles\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "from torch import tensor, Tensor\n",
    "from torch.nn import Sequential, Linear, BCELoss, Sigmoid, Tanh, Module, CrossEntropyLoss, ReLU, Softmax, NLLLoss, KLDivLoss, L1Loss, MSELoss, BCEWithLogitsLoss\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, Adagrad, Adadelta, AdamW, RMSprop, Optimizer\n",
    "from torch.nn.functional import softmax, sigmoid\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self) -> None:\n",
    "        self.history = {}\n",
    "        self.epoch = []\n",
    "\n",
    "def model_forward_func(model: Module):\n",
    "    def forward(x: Tensor):\n",
    "        return sigmoid(model(x))\n",
    "    return lambda x: forward(torch.tensor(x, dtype=torch.float32)).detach().squeeze().numpy()\n",
    "        \n",
    "def to_categorical(x, num_classes=None):\n",
    "    \"\"\"Converts a class vector (integers) to binary class matrix.\n",
    "\n",
    "    E.g. for use with `categorical_crossentropy`.\n",
    "\n",
    "    Args:\n",
    "        x: Array-like with class values to be converted into a matrix\n",
    "            (integers from 0 to `num_classes - 1`).\n",
    "        num_classes: Total number of classes. If `None`, this would be inferred\n",
    "            as `max(x) + 1`. Defaults to `None`.\n",
    "\n",
    "    Returns:\n",
    "        A binary matrix representation of the input as a NumPy array. The class\n",
    "        axis is placed last.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> a = keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n",
    "    >>> print(a)\n",
    "    [[1. 0. 0. 0.]\n",
    "     [0. 1. 0. 0.]\n",
    "     [0. 0. 1. 0.]\n",
    "     [0. 0. 0. 1.]]\n",
    "\n",
    "    >>> b = np.array([.9, .04, .03, .03,\n",
    "    ...               .3, .45, .15, .13,\n",
    "    ...               .04, .01, .94, .05,\n",
    "    ...               .12, .21, .5, .17],\n",
    "    ...               shape=[4, 4])\n",
    "    >>> loss = keras.ops.categorical_crossentropy(a, b)\n",
    "    >>> print(np.around(loss, 5))\n",
    "    [0.10536 0.82807 0.1011  1.77196]\n",
    "\n",
    "    >>> loss = keras.ops.categorical_crossentropy(a, a)\n",
    "    >>> print(np.around(loss, 5))\n",
    "    [0. 0. 0. 0.]\n",
    "    \"\"\"\n",
    "    x = np.array(x, dtype=\"int64\")\n",
    "    input_shape = x.shape\n",
    "\n",
    "    # Shrink the last dimension if the shape is (..., 1).\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "\n",
    "    x = x.reshape(-1)\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(x) + 1\n",
    "    batch_size = x.shape[0]\n",
    "    categorical = np.zeros((batch_size, num_classes))\n",
    "    categorical[np.arange(batch_size), x] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.axes import Axes\n",
    "\n",
    "def plot_decision_boundary(func, X, y, figsize=(7, 5), ax: Axes = None):\n",
    "    amin, bmin = X.min(axis=0) - 0.1\n",
    "    amax, bmax = X.max(axis=0) + 0.1\n",
    "    hticks = np.linspace(amin, amax, 101)\n",
    "    vticks = np.linspace(bmin, bmax, 101)\n",
    "    \n",
    "    aa, bb = np.meshgrid(hticks, vticks)\n",
    "    ab = np.c_[aa.ravel(), bb.ravel()]\n",
    "    c = func(ab)\n",
    "    cc = c.reshape(aa.shape)\n",
    "\n",
    "    cm = plt.colormaps['RdBu']\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "    contour = ax.contourf(aa, bb, cc, cmap=cm, alpha=0.8)\n",
    "    \n",
    "    ax_c = plt.colorbar(contour, ax=ax)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "    \n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    ax.set_xlim(amin, amax)\n",
    "    ax.set_ylim(bmin, bmax)\n",
    "    ax.set_title(\"Decision Boundary\")\n",
    "    return ax\n",
    "\n",
    "def plot_multiclass_decision_boundary(func, X, y, ax: Axes = None):\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n",
    "    cmap = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "    x_map = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = func(x_map)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.colormaps['Spectral'], alpha=0.8)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.colormaps['RdYlBu'])\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_title(\"Decision Boundaries\")\n",
    "    return ax\n",
    "    \n",
    "def plot_data(X, y, figsize=(6, 4), ax: Axes = None):\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(X[y==0, 0], X[y==0, 1], 'or', alpha=0.5, label=0)\n",
    "    ax.plot(X[y==1, 0], X[y==1, 1], 'ob', alpha=0.5, label=1)\n",
    "    ax.set_xlim((min(X[:, 0])-0.1, max(X[:, 0])+0.1))\n",
    "    ax.set_ylim((min(X[:, 1])-0.1, max(X[:, 1])+0.1))\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_pred, y, ax: Axes = None):\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(6, 4))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "    sns.heatmap(pd.DataFrame(confusion_matrix(y, y_pred)), annot=True, fmt='d', cmap='YlGnBu', alpha=0.8, vmin=0, ax=ax)\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(values: dict, y_keys: list, x_keys: list, ax: Axes = None):\n",
    "    assert len(y_keys) == len(x_keys)\n",
    "\n",
    "    assert all(xk in values.keys() for xk in x_keys), f\"x_keys: {x_keys} not in values.keys(): {values.keys()}\"\n",
    "    assert  all(yk in values.keys() for yk in y_keys), f\"y_keys: {y_keys} not in values.keys(): {values.keys()}\"\n",
    "    \n",
    "    for y_k, x_k in zip(y_keys, x_keys):\n",
    "        assert len(values[y_k]) == len(values[x_k])\n",
    "\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(6, 4))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "    # ax.set = plt.get_cmap('tab10')\n",
    "    for y_k, x_k in zip(y_keys, x_keys):\n",
    "        # ic(x_k, y_k)\n",
    "        ax.plot(values[x_k], values[y_k], linewidth=2, markersize=12, alpha=0.7, label=y_k)\n",
    "    \n",
    "    ax.set_xlabel(str(x_keys))\n",
    "    metric_str = str([yk+': '+str(round(values[yk][-1], 3)) for yk in y_keys])\n",
    "    ax.set_title(f\"Metrics: {metric_str}\")\n",
    "    return ax\n",
    "\n",
    "def plot_loss(history: dict, ax: Axes = None):\n",
    "    assert 'loss' in history, \"Loss is not in history\"\n",
    "    assert 'epoch' in history, \"Epoch is not in history\"\n",
    "\n",
    "    # historydf = pd.DataFrame(history['loss'], index=history['epoch'])\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(6, 4))\n",
    "        ax = fig.add_subplot(111)\n",
    "    ax.plot(history['epoch'], history['loss'], label='loss')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_ylim(0, max(1, max(history['loss'])))\n",
    "    ax.legend()\n",
    "    # historydf.plot(ylim=(0, historydf.values.max()), ax=ax)\n",
    "    ax.set_title('Loss: %.3f' % history['loss'][-1])\n",
    "    return ax\n",
    "\n",
    "def plot_loss_accuracy(history: dict, ax: Axes = None):\n",
    "    assert 'loss' in history, \"Loss is not in history\"\n",
    "    assert 'acc' in history, \"Acc is not in history\"\n",
    "    assert 'epoch' in history, \"Epoch is not in history\"\n",
    "\n",
    "    # historydf = pd.DataFrame(history, index=history['epoch'])\n",
    "\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(6, 4))\n",
    "        ax = fig.add_subplot(111)\n",
    "    # hdf = historydf.loc[:, ['loss', 'acc']]\n",
    "    ax.plot(history['epoch'], history['loss'], label='loss')\n",
    "    ax.plot(history['epoch'], history['acc'], label='acc')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylim(0, max(1, max(history['loss'])))\n",
    "    ax.legend()\n",
    "\n",
    "    # hdf.plot(ylim=(0, max(1, hdf.values.max())), ax=ax)\n",
    "    loss = history['loss'][-1]\n",
    "    acc = history['acc'][-1]\n",
    "    ax.set_title('Loss: %.3f, Accuracy: %.3f' % (loss, acc))\n",
    "    return  ax\n",
    "\n",
    "def plot_compare_histories(history_list: list[History], name_list, plot_accuracy=True):\n",
    "    #FIXME: remove history item\n",
    "    dflist = []\n",
    "    for history in history_list:\n",
    "        h = {key: val for key, val in history.history.items() if not key.startswith('val_')}\n",
    "        dflist.append(pd.DataFrame(h, index=history.epoch))\n",
    "\n",
    "    historydf = pd.concat(dflist, axis=1)\n",
    "\n",
    "    metrics = dflist[0].columns\n",
    "    idx = pd.MultiIndex.from_product([name_list, metrics], names=['model', 'metric'])\n",
    "    historydf.columns = idx\n",
    "    \n",
    "    fig = plt.figure(figsize=(6, 8))\n",
    "\n",
    "    ax = fig.add_subplot(211)\n",
    "    historydf.xs('loss', axis=1, level='metric').plot(ylim=(0,1), ax=ax)\n",
    "    ax.set_title(\"Loss\")\n",
    "    \n",
    "    if plot_accuracy:\n",
    "        ax = fig.add_subplot(212)\n",
    "        historydf.xs('acc', axis=1, level='metric').plot(ylim=(0,1), ax=ax)\n",
    "        ax.set_title(\"Accuracy\")\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sine_wave():\n",
    "    c = 3\n",
    "    num = 2400\n",
    "    step = num/(c*4)\n",
    "    np.random.seed(0)\n",
    "    x0 = np.linspace(-c*np.pi, c*np.pi, num)\n",
    "    x1 = np.sin(x0)\n",
    "    noise = np.random.normal(0, 0.1, num) + 0.1\n",
    "    noise = np.sign(x1) * np.abs(noise)\n",
    "    x1  = x1 + noise\n",
    "    x0 = x0 + (np.asarray(range(num)) / step) * 0.3\n",
    "    X = np.column_stack((x0, x1))\n",
    "    y = np.asarray([int((i/step)%2==1) for i in range(len(x0))])\n",
    "    return X, y\n",
    "\n",
    "def make_multiclass(N=500, D=2, K=3, noise=0.2, shuffle= False, shuffle_seed=42, return_idx =False,plot= False):\n",
    "    \"\"\"\n",
    "    N: number of points per class\n",
    "    D: dimensionality\n",
    "    K: number of classes\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    X = np.zeros((N*K, D))\n",
    "    y = np.zeros(N*K)\n",
    "    for j in range(K):\n",
    "        ix = range(N*j, N*(j+1))\n",
    "        # radius\n",
    "        r = np.linspace(0.0,1,N)\n",
    "        # theta\n",
    "        t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*noise\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.seed(shuffle_seed)\n",
    "        idx = np.random.permutation(N*K)\n",
    "        X, y = X[idx], y[idx]\n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.colormaps['RdYlBu'], alpha=0.8)\n",
    "        ax.set_xlim(-1,1)\n",
    "        ax.set_ylim(-1,1)\n",
    "    if shuffle and return_idx:\n",
    "        return X, y, idx\n",
    "    else:\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_data(plot=False, n_samples=1000, n_features=2, n_redundant=0, n_informative=2, random_state=7, n_clusters_per_class=1):\n",
    "    X, y = make_classification(n_samples=n_samples, n_features=n_features, n_redundant=n_redundant, n_informative=n_informative, random_state=random_state, n_clusters_per_class=n_clusters_per_class)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).view(-1,1)\n",
    "    if plot:\n",
    "        plot_data(X, y)\n",
    "    return X, y, X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Sklearn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sklearn_logistic_regression(X, y):\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, y)\n",
    "    print('LR coefficients:', lr.coef_)\n",
    "    print('LR intercept:', lr.intercept_)\n",
    "\n",
    "    plot_data(X, y)\n",
    "\n",
    "    limits = np.array([-2, 2])\n",
    "    boundary = -(lr.coef_[0][0] * limits + lr.intercept_[0]) / lr.coef_[0][1]\n",
    "    plt.plot(limits, boundary, \"g-\", linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binary_accuracy(y_true: Tensor, y_pred: Tensor):\n",
    "    assert y_true.shape == y_pred.shape, f\"y_true: {y_true.shape} != y_pred: {y_pred.shape}\"\n",
    "    assert y_true.shape[1] == 1, f\"y_true: {y_true.shape} != (n, 1)\"\n",
    "    assert y_pred.shape[1] == 1, f\"y_pred: {y_pred.shape} != (n, 1)\"\n",
    "    # y_true = y_true.view(-1)\n",
    "    # y_pred = y_pred.view(-1)\n",
    "    return (y_true == (y_pred > 0)).sum().item() / len(y_true)\n",
    "\n",
    "def compute_multiclass_accuracy(y_true: Tensor, y_pred: Tensor):\n",
    "    assert y_true.shape == y_pred.shape, f\"y_true: {y_true.shape} != y_pred: {y_pred.shape}\"\n",
    "    assert y_true.shape[1] > 1, f\"y_true: {y_true.shape} != (n, >1)\"\n",
    "    assert y_pred.shape[1] > 1, f\"y_pred: {y_pred.shape} != (n, >1)\"\n",
    "\n",
    "    y_pred_d = torch.argmax(y_pred, dim=1)\n",
    "    y_og = torch.argmax(y_true, dim=1)\n",
    "\n",
    "    return(y_pred_d == y_og).sum().item() / len(y_og)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_trainer(model: Module, X_tensor: Tensor, y_tensor: Tensor, optimizer, criterion, epochs: int, accuracy_func):\n",
    "    history = {}\n",
    "    history['loss'] = []\n",
    "    history['acc'] = []\n",
    "    history['epoch'] = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_tensor)\n",
    "        loss = criterion(y_pred, y_tensor)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # y_pred_d = y_pred > 0.5\n",
    "        # acc = (y_pred_d == y_tensor).sum().item() / len(y_tensor)\n",
    "        acc = accuracy_func(y_tensor, y_pred)\n",
    "        # y_pred_d = torch.argmax(y_pred, dim=1)\n",
    "        # y_og = torch.argmax(y_tensor, dim=1)\n",
    "        # acc = (y_pred_d == y_og).sum().item() / len(y_tensor)\n",
    "        # ic(y_pred_d.shape, y_og.shape)\n",
    "        \n",
    "        # ic(acc)\n",
    "        history['loss'].append(loss.item())\n",
    "        history['acc'].append(acc)\n",
    "        history['epoch'].append(epoch)\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch:', epoch, 'Loss:', loss.item(), end='\\r')\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_trainer(model: Module, X_tensor: Tensor, y_tensor: Tensor, optimizer, criterion, epochs: int, accuracy_func, batch_size:int = 256, shuffle=True):\n",
    "    history = {}\n",
    "    history['loss'] = []\n",
    "    history['acc'] = []\n",
    "    history['epoch'] = []\n",
    "    history['step'] = []\n",
    "    history['loss_int'] = []\n",
    "    \n",
    "    step_len = batch_size/(X_tensor.shape[0])\n",
    "    # ic(step_len)\n",
    "    ic(batch_size)\n",
    "    if shuffle:\n",
    "        idx = torch.randperm(X_tensor.shape[0])\n",
    "        X_tensor = X_tensor[idx]\n",
    "        y_tensor = y_tensor[idx]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for step, i in enumerate(range(0, X_tensor.shape[0], batch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            X_batch = X_tensor[i:i+batch_size]\n",
    "            y_batch = y_tensor[i:i+batch_size]\n",
    "            y_pred_batch = model(X_batch)\n",
    "            loss = criterion(y_pred_batch, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            history['loss_int'].append(loss.item())\n",
    "            history['step'].append(epoch + step_len*step)\n",
    "\n",
    "        y_pred = model(X_tensor)\n",
    "        loss = criterion(y_pred, y_tensor)\n",
    "        acc = accuracy_func(y_tensor, y_pred)\n",
    "\n",
    "        history['loss'].append(loss.item())\n",
    "        history['acc'].append(acc)\n",
    "        history['epoch'].append(epoch)\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch:', epoch, 'Loss:', loss.item(), end='\\r')\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logistic_regression_model():\n",
    "    model = Sequential(Linear(2, 1))\n",
    "    criterion = BCEWithLogitsLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.1)\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log regr for simple classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_tensor, y_tensor = get_classification_data(n_samples=1000)\n",
    "model, criterion, optimizer = get_logistic_regression_model()\n",
    "# model, history = batch_trainer(model, X_tensor, y_tensor, optimizer, criterion, 50, compute_binary_accuracy)\n",
    "model, history = minibatch_trainer(model, X_tensor, y_tensor, optimizer, criterion, 50, compute_binary_accuracy)\n",
    "y_pred = (model(X_tensor) > 0.5).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_fig, axs = plt.subplots(1,3 ,figsize=(16, 4), width_ratios=[6,8,6])\n",
    "ax1 = plot_loss_accuracy(history, ax=axs[0])\n",
    "ax2 = plot_decision_boundary(model_forward_func(model), X, y, ax=axs[1])\n",
    "ax3 = plot_confusion_matrix(y_pred, y, ax=axs[2])\n",
    "# print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moons_data(plot=False, n_samples=1024, noise=0.05, random_state=0):\n",
    "    X, y = make_moons(n_samples=n_samples, noise=noise, random_state=random_state)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).view(-1,1)\n",
    "    if plot:\n",
    "        plot_data(X, y)\n",
    "    return X, y, X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, x_tensor, y_tensor = get_moons_data()\n",
    "X.shape, y.shape, x_tensor.shape, y_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR with moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_tensor, y_tensor = get_moons_data()\n",
    "model, criterion, optimizer = get_logistic_regression_model()\n",
    "model, history = batch_trainer(model, X_tensor, y_tensor, optimizer, criterion, 100, compute_binary_accuracy)\n",
    "y_pred = (model(X_tensor) > 0.5).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_fig2, axs = plt.subplots(1,3 ,figsize=(16, 4), width_ratios=[6,8,6])\n",
    "ax1 = plot_loss_accuracy(history, ax=axs[0])\n",
    "ax2 = plot_decision_boundary(model_forward_func(model), X, y, ax=axs[1])\n",
    "ax3 = plot_confusion_matrix(y_pred, y, ax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_nn_model():\n",
    "    model = Sequential(\n",
    "        Linear(2,4),\n",
    "        Tanh(),\n",
    "        Linear(4,2),\n",
    "        Tanh(),\n",
    "        Linear(2,1)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_tensor, y_tensor = get_moons_data()\n",
    "model = get_simple_nn_model()\n",
    "criterion = BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.1)\n",
    "model, history = batch_trainer(model, X_tensor, y_tensor, optimizer, criterion, 100, compute_binary_accuracy)\n",
    "y_pred = (model(X_tensor) > 0).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_fig3, axs = plt.subplots(1,3 ,figsize=(16, 4), width_ratios=[6,8,6])\n",
    "ax1 = plot_loss_accuracy(history, ax=axs[0])\n",
    "ax2 = plot_decision_boundary(model_forward_func(model), X, y, ax=axs[1])\n",
    "ax3 = plot_confusion_matrix(y_pred, y, ax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_circles_data(plot = False):\n",
    "    X, y = make_circles(n_samples=1000, noise=0.05, factor=0.3, random_state=0)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).view(-1,1)\n",
    "    if plot:\n",
    "        plot_data(X, y)\n",
    "    return X, y, X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_tensor, y_tensor = get_circles_data()\n",
    "model = get_simple_nn_model()\n",
    "criterion = BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.1)\n",
    "model, history = batch_trainer(model, X_tensor, y_tensor, optimizer, criterion, 100, compute_binary_accuracy)\n",
    "y_pred = (model(X_tensor) > 0).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_fig3, axs = plt.subplots(1,3 ,figsize=(16, 4), width_ratios=[6,8,6])\n",
    "ax1 = plot_loss_accuracy(history, ax=axs[0])\n",
    "ax2 = plot_decision_boundary(model_forward_func(model), X, y, ax=axs[1])\n",
    "ax3 = plot_confusion_matrix(y_pred, y, ax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 2048\n",
    "X, y = make_multiclass(K=3, noise=0.2, N = data_size)\n",
    "y_cat = to_categorical(y)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_cat, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t = tensor(y, dtype=torch.long)\n",
    "y_oh = nn.functional.one_hot(y_t)\n",
    "X_tensor.shape, y_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutliclass_forward_func_with_softmax(model: Module):\n",
    "    def forward(x):\n",
    "        x = model(x)\n",
    "        x = nn.functional.softmax(x, dim=1)\n",
    "        return torch.argmax(x, dim=1)\n",
    "    return lambda x: forward(tensor(x, dtype=torch.float32)).detach().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(Linear(2, 3))\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "model, history = batch_trainer(model, X_tensor, y_tensor, optimizer, criterion, 100, compute_multiclass_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = mutliclass_forward_func_with_softmax(model)\n",
    "y_pred = fn(X_tensor)\n",
    "master_fig35, axs = plt.subplots(1,3 ,figsize=(16, 4), width_ratios=[6,6,6])\n",
    "ax1 = plot_loss_accuracy(history, ax=axs[0])\n",
    "ax2 = plot_multiclass_decision_boundary(fn , X, y, ax=axs[1])\n",
    "ax3 = plot_confusion_matrix(y_pred, y, ax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiclass_model():\n",
    "    model = Sequential(\n",
    "        Linear(2, 128),\n",
    "        Tanh(),\n",
    "        Linear(128, 64),\n",
    "        Tanh(),\n",
    "        Linear(64, 32),\n",
    "        Tanh(),\n",
    "        Linear(32, 16),\n",
    "        Tanh(),\n",
    "        Linear(16, 3)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiclass_model_small():\n",
    "    model = Sequential(\n",
    "        Linear(2, 16),\n",
    "        Tanh(),\n",
    "        Linear(16, 8),\n",
    "        Tanh(),\n",
    "        Linear(8, 3),\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiclass_model_small2(embed_dim=8):\n",
    "    model = Sequential(\n",
    "        Linear(2, embed_dim),\n",
    "        Tanh(),\n",
    "        Linear(embed_dim, 3)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miniatch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_multiclass_model()\n",
    "# model = get_multiclass_model_small()\n",
    "model = get_multiclass_model_small2(embed_dim=16)\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.1)\n",
    "# optimizer = SGD(model.parameters(), lr=0.1)\n",
    "# model, history = trainer(model, X_tensor, y_tensor, optimizer, criterion, 200)\n",
    "model, history = minibatch_trainer(model, X_tensor, y_tensor, optimizer, criterion, 200, batch_size=data_size//16, accuracy_func=compute_multiclass_accuracy)\n",
    "fn = mutliclass_forward_func_with_softmax(model)\n",
    "y_pred = fn(X_tensor)\n",
    "master_fig4, axs = plt.subplots(1,4 ,figsize=(16, 4), width_ratios=[4,4,6,6])\n",
    "ax1 = plot_metrics(history, y_keys=[ 'loss_int', 'acc'],x_keys=[ 'step','epoch'] , ax=axs[0])\n",
    "ax11 = plot_loss_accuracy(history, ax=axs[1])\n",
    "ax2 = plot_multiclass_decision_boundary(fn , X, y, ax=axs[2])\n",
    "ax3 = plot_confusion_matrix(y_pred, y, ax=axs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_multiclass_model_small2(embed_dim=16)\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.1)\n",
    "# optimizer = SGD(model.parameters(), lr=0.1)\n",
    "model, history = batch_trainer(model, X_tensor, y_tensor, optimizer, criterion, 200, accuracy_func=compute_multiclass_accuracy)\n",
    "fn = mutliclass_forward_func_with_softmax(model)\n",
    "y_pred = fn(X_tensor)\n",
    "master_fig4, axs = plt.subplots(1,3 ,figsize=(16, 4), width_ratios=[6,6,6])\n",
    "# ax1 = plot_metrics(history, y_keys=[ 'loss_int', 'acc'],x_keys=[ 'step','epoch'] , ax=axs[0])\n",
    "ax1 = plot_loss_accuracy(history, ax=axs[0])\n",
    "ax2 = plot_multiclass_decision_boundary(fn , X, y, ax=axs[1])\n",
    "ax3 = plot_confusion_matrix(y_pred, y, ax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = dict(model.named_parameters())\n",
    "n_cols = 4\n",
    "n_rows = (len(state_dict)-1)//n_cols +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_xlabels(ax: Axes, limit=20):\n",
    "    ylabs = ax.get_xticklabels()\n",
    "    len_ylab = len(ylabs)\n",
    "    if len_ylab> limit:\n",
    "        ylab_new = ['' for i in range(len_ylab)]\n",
    "        reduction = len_ylab//limit\n",
    "        ylab_new[::reduction+1] = ylabs[::reduction+1]\n",
    "        ax.set_xticklabels(ylab_new)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "fig = plt.figure(figsize=(5*n_cols, 4*n_rows))\n",
    "for i, (key, val) in enumerate(state_dict.items()):\n",
    "    ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "    sns.histplot(val.detach().flatten().numpy(),kde=True, palette='viridis', ax=ax)\n",
    "    ax.set_title(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:\n",
    "def plot_weights_barplot(weights: dict[str, Tensor], ax: Axes = None):\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(6, 4))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "    keys = list(weights.keys())\n",
    "    vals = [weights[key].detach().numpy().flatten() for key in keys]\n",
    "    sns.barplot(keys, vals)\n",
    "    ax.set_title('Weights')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))\n",
    "fig = plt.figure(figsize=(5*n_cols, 4*n_rows))\n",
    "for i, (key, val) in enumerate(state_dict.items()):\n",
    "    # ax = axs[i//3, i%3]\n",
    "    ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "    sns.barplot(val.detach().flatten().numpy(), ax=ax, palette='viridis')\n",
    "    ax = sparsify_xlabels(ax)\n",
    "    ax.set_title(key)\n",
    "    ax.set_xlabel('Neuron index')\n",
    "    ax.set_ylabel('Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 2048\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "data_map = {}\n",
    "ids = {}\n",
    "for seed in seeds:\n",
    "    X, y, idx = make_multiclass(K=3, noise=0.2, N = data_size, shuffle=True, shuffle_seed=seed, return_idx=True)\n",
    "    ids[seed] = idx\n",
    "    y_cat = to_categorical(y)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_cat, dtype=torch.float32)\n",
    "    data_map[seed] = (X, y, X_tensor, y_tensor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_trainer(data_map: dict, model_getter, accuracy_func):\n",
    "    seeds = data_map.keys()\n",
    "    model_map = {s: model_getter() for s in seeds}\n",
    "    histories = {}\n",
    "    for seed in seeds:\n",
    "        X, y, X_tensor, y_tensor = data_map[seed]\n",
    "        model = model_map[seed]\n",
    "        criterion = CrossEntropyLoss()\n",
    "        optimizer = Adam(model.parameters(), lr=0.1)\n",
    "        model, history = minibatch_trainer(model, X_tensor, y_tensor, optimizer, criterion, 200, accuracy_func=accuracy_func)\n",
    "        model_map[seed] = model\n",
    "        histories[seed] =  history\n",
    "    return model_map, histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "model_getter = partial(get_multiclass_model_small2, embed_dim=8)\n",
    "model_map, histories = ensemble_trainer(data_map, model_getter, compute_multiclass_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {s: get_multiclass_model_small2(embed_dim=8) for s in seeds}\n",
    "histories = {}\n",
    "for s in seeds:\n",
    "    model = model_map[s]\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.1)\n",
    "    # optimizer = SGD(model.parameters(), lr=0.1)\n",
    "    _, _, X_tensor, y_tensor = data_map[s]\n",
    "    model, history = minibatch_trainer(model, X_tensor, y_tensor, optimizer, criterion, 200, batch_size=data_size//16, accuracy_func=compute_multiclass_accuracy)\n",
    "    model_map[s] = model \n",
    "    histories[s] = history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_fig, axs = plt.subplots(1,5 ,figsize=(20, 4))\n",
    "for i, s in enumerate(seeds):\n",
    "    ax = axs[i]\n",
    "    ax1 = plot_loss_accuracy(histories[s], ax=ax)\n",
    "    ax1.set_title(f\"Seed: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_fig, axs = plt.subplots(1,5 ,figsize=(20, 4))\n",
    "\n",
    "for i, s in enumerate(seeds):\n",
    "    model = model_map[s]\n",
    "    X, y, _, _ = data_map[s]\n",
    "    fn = mutliclass_forward_func_with_softmax(model)\n",
    "    ax = axs[i]\n",
    "    ax2 = plot_multiclass_decision_boundary(fn , X, y, ax=ax)\n",
    "    ax2.set_title(f\"Seed: {s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Barplots for weights\n",
    "\n",
    "# n_rows, n_cols = (len(state_dict)-1)//4 +1, 4\n",
    "master_fig, axs = plt.subplots(5,4 ,figsize=(16, 20))\n",
    "for i, s in enumerate(seeds):\n",
    "    weight_dict = dict(model_map[s].named_parameters())\n",
    "    for j, (key, val) in enumerate(weight_dict.items()):\n",
    "        ax = axs[i, j]\n",
    "        sns.barplot(val.detach().flatten().numpy(), ax=ax, palette='viridis')\n",
    "        ax = sparsify_xlabels(ax)\n",
    "        ax.set_title(f\"Seed: {s}, Layer: {key}\")\n",
    "    # ax.set_title(key)\n",
    "    axs[i,0].set_ylabel('Value')\n",
    "\n",
    "for k in range(4):\n",
    "    axs[-1, k].set_xlabel('Neuron index')\n",
    "# axs[-1, k].set_xlabel('Neuron index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram of weights\n",
    "# fig = plt.figure(figsize=(5*n_cols, 4*n_rows))\n",
    "master_fig, axs = plt.subplots(5,4 ,figsize=(16, 20))\n",
    "for i, s in enumerate(seeds):\n",
    "    weight_dict = dict(model_map[s].named_parameters())\n",
    "    for j, (key, val) in enumerate(weight_dict.items()):\n",
    "        # ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        ax = axs[i, j]\n",
    "        sns.histplot(val.detach().flatten().numpy(),kde=True, palette='viridis', ax=ax)\n",
    "        ax.set_title(f\"Seed: {s}, Layer: {key}\")\n",
    "\n",
    "        ax.set_title(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_fig, axs = plt.subplots(1,4 ,figsize=(16, 4))\n",
    "for i, s in enumerate(seeds):\n",
    "    weight_dict = dict(model_map[s].named_parameters())\n",
    "    for j, (key, val) in enumerate(weight_dict.items()):\n",
    "        # ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        ax = axs[j]\n",
    "        sns.histplot(val.detach().flatten().numpy(),kde=True, palette='viridis', ax=ax, element='step', fill=True)\n",
    "        ax.set_title(f\"Seed: {s}, Layer: {key}\")\n",
    "\n",
    "        ax.set_title(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_merge_models(models: list[Module]):\n",
    "    merged_model = deepcopy(models[0])\n",
    "    # merged_param_itr = merged_model.named_parameters()\n",
    "    for key, param in merged_model.named_parameters():\n",
    "        stacked = torch.stack([m.get_parameter(key) for m in models], dim=0)\n",
    "        param.data = torch.mean(stacked, dim=0)\n",
    "    ic(param.data[-1])\n",
    "\n",
    "    return merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = simple_merge_models([model_map[s] for s in seeds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = mutliclass_forward_func_with_softmax(merged)\n",
    "ax2 = plot_multiclass_decision_boundary(fn , X, y)\n",
    "ax2.set_title(f\"Merged model decision boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged weights visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_fig, axs = plt.subplots(1,4 ,figsize=(16, 4))\n",
    "# for i, s in enumerate(seeds):\n",
    "merged_weight_dict = dict(merged.named_parameters())\n",
    "for j, (key, val) in enumerate(merged_weight_dict.items()):\n",
    "    ax = axs[j]\n",
    "    sns.barplot(val.detach().flatten().numpy(), ax=ax, palette='viridis')\n",
    "    ax = sparsify_xlabels(ax)\n",
    "    ax.set_title(f\"Merged Layer: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_batch = merged(X_tensor)\n",
    "# y_pred_batch = model_map[0](X_tensor)\n",
    "criterion(y_pred_batch, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, X_tensor):\n",
    "    y_preds = [model(X_tensor) for model in models]\n",
    "    y_pred = torch.stack(y_preds, dim=0).mean(dim=0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_fig, axs = plt.subplots(1,5 ,figsize=(20, 4))\n",
    "\n",
    "for i, s in enumerate(seeds):\n",
    "    model = model_map[s]\n",
    "    X, y, X_tensor, _ = data_map[s]\n",
    "    y_pred = fn(X_tensor)\n",
    "    ax3 = plot_confusion_matrix(y_pred, y, ax=axs[i])\n",
    "    ax3.set_title(f\"Seed: {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_federated_learning(data_map, model, accuracy_func, epochs=1, rounds=200):\n",
    "    num_clients = len(data_map)\n",
    "    model_map = {i: deepcopy(model) for i in range(num_clients)}\n",
    "    histories = {}\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    for r in range(rounds):\n",
    "        for k in range(num_clients):\n",
    "            X, y, X_tensor, y_tensor = data_map[k]\n",
    "            optimizer = Adam(model_map[k].parameters(), lr=0.1)\n",
    "            model, history = minibatch_trainer(model_map[k], X_tensor, y_tensor, optimizer, criterion, epochs, accuracy_func)\n",
    "            model_map[k] = model\n",
    "            histories[k] = history\n",
    "        merged = simple_merge_models([model_map[s] for s in range(num_clients)])\n",
    "        model_map = {i: deepcopy(merged) for i in range(num_clients)}\n",
    "\n",
    "    return model_map, histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iid_data_split(data_map, num_clients):\n",
    "    data_size = len(data_map[0][0])\n",
    "    split_size = data_size//num_clients\n",
    "    split_data_map = {}\n",
    "    for i in range(num_clients):\n",
    "        split_data_map[i] = (data_map[0][i*split_size:(i+1)*split_size], data_map[1][i*split_size:(i+1)*split_size])\n",
    "    return split_data_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_sine_wave()\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\n",
    "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n",
    "_ = ax.legend(\n",
    "    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused but required import for doing 3d projections with matplotlib < 3.2\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\", elev=-150, azim=110)\n",
    "\n",
    "X_reduced = PCA(n_components=3).fit_transform(iris.data)\n",
    "ax.scatter(\n",
    "    X_reduced[:, 0],\n",
    "    X_reduced[:, 1],\n",
    "    X_reduced[:, 2],\n",
    "    c=iris.target,\n",
    "    s=40,\n",
    ")\n",
    "\n",
    "ax.set_title(\"First three PCA dimensions\")\n",
    "ax.set_xlabel(\"1st Eigenvector\")\n",
    "ax.xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"2nd Eigenvector\")\n",
    "ax.yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"3rd Eigenvector\")\n",
    "ax.zaxis.set_ticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()\n",
    "history.history['loss'] = []\n",
    "history.history['acc'] = []\n",
    "for e in range(50):\n",
    "    \n",
    "    y_pred_tensor = torch.squeeze(model(X_tensor))\n",
    "    loss = criterion(y_pred_tensor, y_tensor)\n",
    "    y_pred = y_pred_tensor.round().detach().numpy()\n",
    "    \n",
    "    if e % 10 == 0:\n",
    "        print('Epoch:', e, 'Loss:', loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total += y_tensor.size(0)\n",
    "    correct += np.sum(y_pred == y)\n",
    "    accuracy = correct/total\n",
    "\n",
    "    history.history['loss'].append(loss.item())\n",
    "    history.history['acc'].append(accuracy)\n",
    "\n",
    "    history.epoch.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
