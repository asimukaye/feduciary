%YAML 1.2
---
# Experiment mode: Switch between debug and experiment methods
# mode: fedstdev_centralized
# mode: base_centralized
mode: debug
# Description: A string to describe the intent of the run
# desc: 'FedStdev centralized run with iid data, pilot run for centralized'
desc: ' Fedstev centralized; one noisy, fixed sigma by mu logic'

# desc: 'FedAvg rerun with fix delta normalize, iid, with delta_normalize'

defaults:
  - base_config
  - hydra: custom
  # - server: fedstdev
  # - server: fedavg
  # - strategy/cfg: base_strategy
  
  # - client: fedstdev
  # - client: base
  # - train: base_trainer
  - _self_

# CONFIG OVERRIDES
# client:
#   cfg:
#     batch_size: 512
#     epochs: 1
#     device: auto
client:
  _target_: src.client.baseflowerclient.BaseFlowerClient
  _partial_: true
  cfg:
    epochs: 5
    start_epoch: 0
    device: auto
    batch_size: 32
    lr: 0.01
    lr_decay: 0.977
    optimizer:
      _target_: torch.optim.SGD
      _partial_: true
      lr: ${client.cfg.lr}
    criterion:
      _target_: torch.nn.CrossEntropyLoss
    lr_scheduler:
      _target_: torch.optim.lr_scheduler.ExponentialLR
      _partial_: true
      gamma: ${client.cfg.lr_decay}

    shuffle: false
    eval_metrics: ['acc1']
# server:
#   cfg:
#     delta_normalize: false

server:
  _target_: src.server.baseflowerserver.BaseFlowerServer
  _partial_: true
  cfg:
    sampling_fraction: 1.0
    # server_lr: 1.0
    # update_rule: param_average
    rounds: ${simulator.num_rounds}
    eval_type: both
    eval_fraction: 1.0
    eval_batch_size: 64
    eval_every: 1
  train_cfg: ${client.cfg}
    # weighting_strategy: tanh
    # weighting_strategy: tanh_sigma_by_mu
# ########## OVERRIDES END HERE ##########

simulator:
  seed: 42
  use_tensorboard: false
  use_wandb: true
  save_csv: false
  out_prefix: ''
  num_clients: 4
  num_rounds: 30
  checkpoint_every: 20
  # Simulator Mode: Takes arguments 'federated', 'standalone' or 'centralized', 'flower'
  # mode: centralized
  mode: federated
  # mode: flower 
  flwr_resources:
    num_cpus: 2
    num_gpus: 0.2 #TODO: Couple this with device init


dataset:
  name: CIFAR10
  data_path: data/
  subsample: false
  subsample_fraction: 1.0
  transforms:
    resize:
      _target_: torchvision.transforms.Resize
      size: 
        - 28
        - 28

  split_conf:
    # split_type: one_label_fli`pped_client
    # split_type: one_imbalanced_client
    # split_type: iid
    split_type: one_noisy_client
    num_splits: ${simulator.num_clients}
    # Noise parameter is specific to one patho client
    test_fraction: 0.2 # Client train-test split

    noise:
      mu: 0.0
      sigma: 0.5
      flip_percent: 1.0
  

strategy:
  _target_: src.strategy.basestrategy.BaseStrategy
  # _partial_: true
  cfg:
    train_fraction: 1.0
    eval_fraction: 1.0
  

# train_cfg:
  # epochs: 5
  # device: auto
  # batch_size: 32
  # lr: 0.01
  # lr_decay: 0.977
  # optimizer:
  #   _target_: torch.optim.SGD
  #   _partial_: true
  #   lr: ${client.cfg.lr}
  # criterion:
  #   _target_: torch.nn.CrossEntropyLoss
  # lr_scheduler:
  #   _target_: torch.optim.lr_scheduler.ExponentialLR
  #   _partial_: true
  #   gamma: ${client.cfg.lr_decay}

model:
  name: TwoCNN
  init_type: xavier
  init_gain: 1.0
  dropout: 0.0
  model_spec:           # Optional model spec provided for quick tuning
    _target_: src.models.twocnn.TwoCNN
    _partial_: false
    hidden_size: 200
    # Option to override in_channels and num_classes. Otherwise auto set by the dataset
    in_channels: null
    num_classes: null 


# Log Configuration: Additional configurations to be copied in the results summary section. Must be a valid configuration from the config tree.
log_conf: []

...