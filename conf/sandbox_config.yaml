%YAML 1.2
---

# mode: fedgradstd_trials
# mode: debug
mode: fedgradstd_bloodmnist_trials
# mode: sgd_variance_trials
# mode: cgsv
# desc: 'Centralized run with three label flipped client'
# desc: 'Centralized run with three noisy client'
desc: 'Fedgradstd central run with bloodmnist, 0% label flip'
# desc: 'Fedgradstd dry runs for analyzing different metrics with noisy data and 25 % data'
# desc: 'Fedgradstd dry runs for analyzing different metrics with 75 % label flipped data'

# desc: 'sgd_variance dry runs for analyzing different metrics with iid data and 25 % data'

# desc: 'sgd_variance dry runs for analyzing different metrics with noisy data and 25 % data'

# desc: 'sgd_variance dry runs for analyzing different metrics with label flip data and 25 % data'
# desc: Fedopt run with three label flipped clients
# desc: CGSV run with three label fliped clients
# desc: 'Fedavg run with three noisy clients'
# desc: 'Fedavg manual run with three noisy clients'

# desc: 'Fedavg manual run with three label flipped client'

defaults:
  - base_config
  - hydra: custom
  - server: baseflower

  # - strategy: base
  # - strategy: fedavgmanual
  # - strategy: fedstdev
  - strategy: fedgradstd
  # - strategy: fedopt
  # - strategy: cgsv

  # - client: fedstdev
  - client: fedgradstd
  # - client: baseflower
  - _self_

simulator:
  # Simulator Mode: Takes arguments 'federated', 'standalone' or 'centralized', 'flower'
  # mode: standalone
  # mode: centralized
  mode: federated
  # mode: flower
  seed: 42
  use_tensorboard: false
  use_wandb: true
  save_csv: true
  out_prefix: ''
  num_clients: 6
  num_rounds: 150
  checkpoint_every: 10
  eval_type: both
  eval_every: 1
  flwr_resources:
    num_cpus: 3
    num_gpus: 0.5 #TODO: Couple this with device init

train_cfg:
  epochs: 1
  device: cuda:5
  batch_size: 128
  eval_batch_size: 256
  lr: 0.01
  lr_decay: 0.977
  optimizer:
    _target_: torch.optim.SGD
    _partial_: true
    lr: ${train_cfg.lr}
  criterion:
    _target_: torch.nn.CrossEntropyLoss
  lr_scheduler:
    _target_: torch.optim.lr_scheduler.ExponentialLR
    _partial_: true
    gamma: ${train_cfg.lr_decay}
  metric_cfg:
    eval_metrics: ['acc1']
    file_prefix: ''
    log_to_file: false


dataset:
  # dataset_family: torchvision
  dataset_family: medmnist
  # name: CIFAR10
  name: bloodmnist
  data_path: ../data/
  subsample: false
  subsample_fraction: 1.0 # Fraction of the dataset to use. Only works when subsample is true
  federated: true

  transforms:
    resize:
      _target_: torchvision.transforms.Resize
      size: 
        - 28
        - 28

  split_conf:
    # split_type: one_label_flipped_client
    split_type: n_label_flipped_clients
    # split_type: n_noisy_clients

    # split_type: one_imbalanced_client
    # split_type: iid
    # split_type: one_noisy_client
    num_splits: ${simulator.num_clients}
    # Noise parameter is specific to one patho client
    # Obsolete now
    num_class_per_client: 3 # classes per client for patho split type
    num_patho_clients: 3
    dirichlet_alpha: 1.0
    noise:
      mu: 0.0
      sigma: 0.5
      flip_percent: 0.0


model:
  name: TwoCNN
  init_type: xavier
  init_gain: 1.0
  dropout: 0.0
  model_spec:           # Optional model spec provided for quick tuning
    _target_: feduciary.models.twocnn.TwoCNN
    _partial_: false
    hidden_size: 200
    # Option to override in_channels and num_classes. Otherwise auto set by the dataset
    in_channels: null
    num_classes: null 


# CONFIG OVERRIDES
# client:
#   train_cfg:
# #     epochs: 1
#     device: auto


# ########## OVERRIDES END HERE ##########
# Log Configuration: Additional configurations to be copied in the results summary section. Must be a valid configuration from the config tree.
log_conf: []

...